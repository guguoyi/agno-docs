---
title: Comparison Accuracy Evaluation
description: Exmaple showing how to evaluate agent accuracy on comparison tasks.
---


<Steps>

  <Step title="Create a Python file">
    ```python accuracy_comparison.py
    from typing import Optional

    from agno.agent import Agent
    from agno.eval.accuracy import AccuracyEval, AccuracyResult
    from agno.models.openai import OpenAIResponses
    from agno.tools.calculator import CalculatorTools

    evaluation = AccuracyEval(
        name="Comparison Evaluation",
        model=OpenAIResponses(id="gpt-5.2"),
        agent=Agent(
            model=OpenAIResponses(id="gpt-5.2"),
            tools=[CalculatorTools()],
            instructions="You must use the calculator tools for comparisons.",
        ),
        input="9.11 and 9.9 -- which is bigger?",
        expected_output="9.9",
        additional_guidelines="Its ok for the output to include additional text or information relevant to the comparison.",
    )

    result: Optional[AccuracyResult] = evaluation.run(print_results=True)
    assert result is not None and result.avg_score >= 8
    ```
  </Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Install dependencies">
    ```bash
    uv pip install -U openai agno
    ```
  </Step>

  <Step title="Export your OpenAI API key">

    <CodeGroup>

    ```bash Mac/Linux
      export OPENAI_API_KEY="your_openai_api_key_here"
    ```

    ```bash Windows
      $Env:OPENAI_API_KEY="your_openai_api_key_here"
    ```
    </CodeGroup>
  </Step>

  <Step title="Run Agent">
    ```bash
    python accuracy_comparison.py
    ```
  </Step>

</Steps>
