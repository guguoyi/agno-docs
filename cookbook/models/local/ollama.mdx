---
title: Ollama
description: Run open-source models locally with Ollama.
---

Ollama lets you run models like Llama, Mistral, and Qwen locally. No data leaves your machine.

```python
from agno.agent import Agent
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1:8b"),
    markdown=True,
)

agent.print_response("Explain quantum computing", stream=True)
```

## Tool Use

```python
from agno.agent import Agent
from agno.models.ollama import Ollama
from agno.tools.websearch import WebSearchTools

agent = Agent(
    model=Ollama(id="llama3.2:latest"),
    tools=[WebSearchTools()],
    markdown=True,
)

agent.print_response("What's happening in France?", stream=True)
```

## Vision

```python
from pathlib import Path
from agno.agent import Agent
from agno.media import Image
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.2-vision"),
    markdown=True,
)

agent.print_response(
    "What's in this image?",
    images=[Image(filepath=Path("image.png"))],
)
```

## Structured Output

```python
from pydantic import BaseModel, Field
from agno.agent import Agent
from agno.models.ollama import Ollama

class Recipe(BaseModel):
    name: str = Field(..., description="Recipe name")
    ingredients: list[str] = Field(..., description="List of ingredients")
    instructions: list[str] = Field(..., description="Cooking steps")

agent = Agent(
    model=Ollama(id="llama3.2"),
    output_schema=Recipe,
)

agent.print_response("Create a recipe for chocolate chip cookies")
```

## Run Examples

```bash
# Start Ollama server
ollama serve

# Pull a model
ollama pull llama3.1:8b

git clone https://github.com/agno-agi/agno.git
cd agno/cookbook/90_models/ollama

python basic.py
python tool_use.py
```
